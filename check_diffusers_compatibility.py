#!/usr/bin/env python3
"""
æ£€æŸ¥diffusersåº“å…¼å®¹æ€§å’ŒStable Diffusionå¯ç”¨æ€§
"""

import sys
import torch
import subprocess
import importlib.util

def check_package_version(package_name):
    """æ£€æŸ¥åŒ…ç‰ˆæœ¬"""
    try:
        spec = importlib.util.find_spec(package_name)
        if spec is None:
            return None, "Not installed"
        
        module = importlib.import_module(package_name)
        version = getattr(module, '__version__', 'Unknown')
        return version, "OK"
    except Exception as e:
        return None, str(e)

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒå…¼å®¹æ€§"""
    print("ğŸ”§ Environment Compatibility Check")
    print("=" * 50)
    
    # Pythonç‰ˆæœ¬
    python_version = sys.version.split()[0]
    print(f"Python: {python_version}")
    
    # PyTorch
    torch_version, torch_status = check_package_version('torch')
    print(f"PyTorch: {torch_version} ({torch_status})")
    
    if torch.cuda.is_available():
        print(f"CUDA: {torch.version.cuda}")
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB")
    else:
        print("CUDA: Not available")
    
    # å…³é”®åŒ…æ£€æŸ¥
    packages = [
        'diffusers',
        'transformers', 
        'accelerate',
        'safetensors',
        'PIL',
        'numpy'
    ]
    
    print("\nğŸ“¦ Package Versions:")
    for pkg in packages:
        version, status = check_package_version(pkg)
        print(f"  {pkg}: {version} ({status})")
    
    return True

def test_stable_diffusion_basic():
    """æµ‹è¯•Stable DiffusionåŸºæœ¬åŠŸèƒ½"""
    print("\nğŸ¨ Testing Stable Diffusion...")
    
    try:
        from diffusers import StableDiffusionPipeline
        print("âœ… diffusers import successful")
        
        # æ£€æŸ¥å¯ç”¨æ¨¡å‹
        model_id = "runwayml/stable-diffusion-v1-5"
        print(f"ğŸ”§ Testing model: {model_id}")
        
        # åˆ›å»ºpipeline (ä¸åŠ è½½æƒé‡ï¼Œåªæµ‹è¯•å…¼å®¹æ€§)
        pipe = StableDiffusionPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            use_safetensors=True,
            safety_checker=None,  # ç¦ç”¨å®‰å…¨æ£€æŸ¥èŠ‚çœå†…å­˜
            requires_safety_checker=False
        )
        print("âœ… Pipeline creation successful")
        
        if torch.cuda.is_available():
            print("ğŸ”§ Moving to GPU...")
            pipe = pipe.to("cuda")
            print(f"ğŸ”§ GPU memory after loading: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
        
        # æµ‹è¯•ç”Ÿæˆ
        print("ğŸ¨ Testing generation...")
        with torch.no_grad():
            prompt = "a simple test image"
            image = pipe(
                prompt, 
                num_inference_steps=10,  # å¿«é€Ÿæµ‹è¯•
                guidance_scale=7.5,
                height=512,
                width=512
            ).images[0]
            
            print(f"âœ… Generation successful: {image.size}")
            if torch.cuda.is_available():
                print(f"ğŸ”§ GPU memory after generation: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
        
        return True
        
    except Exception as e:
        print(f"âŒ Stable Diffusion test failed: {e}")
        return False

def test_lightweight_sd():
    """æµ‹è¯•è½»é‡çº§Stable Diffusioné…ç½®"""
    print("\nğŸª¶ Testing lightweight Stable Diffusion...")
    
    try:
        from diffusers import StableDiffusionPipeline
        
        # ä½¿ç”¨æ›´å°çš„é…ç½®
        pipe = StableDiffusionPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16,
            use_safetensors=True,
            safety_checker=None,
            requires_safety_checker=False
        )
        
        if torch.cuda.is_available():
            pipe = pipe.to("cuda")
            
            # å¯ç”¨å†…å­˜ä¼˜åŒ–
            pipe.enable_attention_slicing()
            pipe.enable_model_cpu_offload()
            print("âœ… Memory optimizations enabled")
        
        # æµ‹è¯•å°å°ºå¯¸ç”Ÿæˆ
        with torch.no_grad():
            image = pipe(
                "test pattern",
                num_inference_steps=5,
                guidance_scale=5.0,
                height=256,  # æ›´å°å°ºå¯¸
                width=256
            ).images[0]
            
            print(f"âœ… Lightweight generation successful: {image.size}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Lightweight test failed: {e}")
        return False

def recommend_setup():
    """æ¨èè®¾ç½®"""
    print("\nğŸ’¡ Recommendations:")
    print("=" * 50)
    
    # æ£€æŸ¥diffusersç‰ˆæœ¬
    diffusers_version, _ = check_package_version('diffusers')
    
    if diffusers_version:
        major, minor = diffusers_version.split('.')[:2]
        if int(major) == 0 and int(minor) >= 30:
            print("âœ… diffusers version is recent and compatible")
        else:
            print("âš ï¸  Consider upgrading diffusers:")
            print("   pip install -U diffusers")
    
    print("\nğŸ¯ For your micro-Doppler project:")
    print("1. Use Stable Diffusion v1.5 (most stable)")
    print("2. Enable memory optimizations")
    print("3. Start with 256x256 images")
    print("4. Use float16 precision")
    print("5. Consider fine-tuning on your dataset")

def main():
    print("ğŸš€ Diffusers Compatibility Check")
    print("=" * 50)
    
    # åŸºæœ¬ç¯å¢ƒæ£€æŸ¥
    check_environment()
    
    # æµ‹è¯•Stable Diffusion
    if torch.cuda.is_available():
        sd_basic = test_stable_diffusion_basic()
        if not sd_basic:
            print("\nğŸ”§ Trying lightweight configuration...")
            test_lightweight_sd()
    else:
        print("\nâš ï¸  No CUDA available, skipping GPU tests")
    
    # æ¨èè®¾ç½®
    recommend_setup()

if __name__ == "__main__":
    main()
