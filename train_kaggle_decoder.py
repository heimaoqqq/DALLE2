"""
Kaggleä¸“ç”¨è®­ç»ƒè„šæœ¬ - å¾®å¤šæ™®å‹’DALLE2è§£ç å™¨è®­ç»ƒ
é€‚é…Kaggleç¯å¢ƒå’Œæ•°æ®é›†ç»“æ„: /kaggle/input/dataset/ID_1 åˆ° ID_31
"""

import os
import json
import argparse
from pathlib import Path
from datetime import datetime
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from accelerate import Accelerator, DistributedDataParallelKwargs
from tqdm import tqdm

from dalle2_pytorch import Unet, Decoder, OpenClipAdapter
from dalle2_pytorch.trainer import DecoderTrainer
from dalle2_pytorch.dataloaders import create_micro_doppler_dataloader
from dalle2_pytorch.vqgan_vae import VQGanVAE, NullVQGanVAE


def parse_args():
    parser = argparse.ArgumentParser(description='Train Micro-Doppler DALLE2 Decoder on Kaggle')
    
    # Kaggleæ•°æ®è·¯å¾„ (å›ºå®š)
    parser.add_argument('--data_root', type=str, default='/kaggle/input/dataset',
                        help='Kaggle dataset root directory')
    parser.add_argument('--output_dir', type=str, default='/kaggle/working/outputs',
                        help='Output directory for models and logs')

    # æ•°æ®å‚æ•°
    parser.add_argument('--num_users', type=int, default=31,
                        help='Number of users (ID_1 to ID_31)')
    parser.add_argument('--image_size', type=int, default=256,
                        help='Image size (assumes square images)')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--dim', type=int, default=128,
                        help='Base dimension for U-Net')
    parser.add_argument('--dim_mults', type=int, nargs='+', default=[1, 2, 4, 8],
                        help='Dimension multipliers for U-Net layers')
    parser.add_argument('--channels', type=int, default=3,
                        help='Number of image channels')
    parser.add_argument('--timesteps', type=int, default=1000,
                        help='Number of diffusion timesteps')
    parser.add_argument('--use_vqgan', action='store_true',
                        help='Use VQ-GAN VAE for latent diffusion')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=8,
                        help='Batch size (Kaggle GPU memory limited)')
    parser.add_argument('--num_workers', type=int, default=2,
                        help='Number of data loader workers')
    parser.add_argument('--lr', type=float, default=3e-4,
                        help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=1e-2,
                        help='Weight decay')
    parser.add_argument('--epochs', type=int, default=50,
                        help='Number of training epochs (reduced for Kaggle)')
    parser.add_argument('--save_every', type=int, default=10,
                        help='Save model every N epochs')
    parser.add_argument('--sample_every', type=int, default=5,
                        help='Generate samples every N epochs')
    
    # EMAå‚æ•°
    parser.add_argument('--ema_beta', type=float, default=0.99,
                        help='EMA decay rate')
    parser.add_argument('--ema_update_after_step', type=int, default=500,
                        help='Start EMA updates after N steps')
    parser.add_argument('--ema_update_every', type=int, default=10,
                        help='Update EMA every N steps')
    
    # CLIPå‚æ•°
    parser.add_argument('--clip_model', type=str, default='ViT-B/32',
                        help='CLIP model to use')
    
    # å®éªŒå‚æ•°
    parser.add_argument('--experiment_name', type=str, default=None,
                        help='Experiment name (default: timestamp)')
    parser.add_argument('--resume_from', type=str, default=None,
                        help='Resume training from checkpoint')
    
    return parser.parse_args()


def check_kaggle_environment():
    """æ£€æŸ¥Kaggleç¯å¢ƒå’Œæ•°æ®é›†"""
    print("ğŸ” Checking Kaggle environment...")
    
    # æ£€æŸ¥æ˜¯å¦åœ¨Kaggleç¯å¢ƒ
    if not os.path.exists('/kaggle'):
        print("âš ï¸  Warning: Not running in Kaggle environment")
        return False
    
    # æ£€æŸ¥æ•°æ®é›†è·¯å¾„
    data_path = Path('/kaggle/input/dataset')
    if not data_path.exists():
        print(f"âŒ Dataset not found at {data_path}")
        return False
    
    # æ£€æŸ¥ç”¨æˆ·æ–‡ä»¶å¤¹
    user_folders = []
    for i in range(1, 32):  # ID_1 to ID_31
        folder = data_path / f"ID_{i}"
        if folder.exists():
            user_folders.append(folder)
            # ç»Ÿè®¡å›¾åƒæ•°é‡
            image_count = len(list(folder.glob('*.png'))) + len(list(folder.glob('*.jpg')))
            print(f"âœ… Found ID_{i} with {image_count} images")
        else:
            print(f"âš ï¸  Missing ID_{i}")
    
    print(f"ğŸ“Š Total user folders found: {len(user_folders)}/31")
    return len(user_folders) > 0


def create_model(args):
    """åˆ›å»ºè§£ç å™¨æ¨¡å‹"""
    print("ğŸ—ï¸  Creating decoder model...")
    
    # åˆ›å»ºCLIPé€‚é…å™¨
    clip = OpenClipAdapter(args.clip_model)
    
    # åˆ›å»ºVQ-GAN VAE (å¦‚æœæŒ‡å®š)
    if args.use_vqgan:
        print("ğŸ¨ Using VQ-GAN VAE for latent diffusion")
        vae = VQGanVAE(
            dim=32,
            image_size=args.image_size,
            channels=args.channels,
            layers=3,
            vq_codebook_dim=256,
            vq_codebook_size=1024,
            vq_decay=0.8,
            use_vgg_and_gan=True
        )
    else:
        print("ğŸ–¼ï¸  Using pixel-space diffusion")
        vae = NullVQGanVAE(channels=args.channels)
    
    # åˆ›å»ºU-Net
    unet = Unet(
        dim=args.dim,
        image_embed_dim=512,  # CLIP embedding dimension
        cond_dim=128,
        channels=args.channels,
        dim_mults=tuple(args.dim_mults),
        cond_on_image_embeds=True,
        cond_on_text_encodings=False,  # ä¸ä½¿ç”¨æ–‡æœ¬æ¡ä»¶
        self_attn=True,
        attn_heads=8,
        attn_dim_head=64,
        cosine_sim_cross_attn=True,
        cosine_sim_self_attn=True
    )
    
    # åˆ›å»ºè§£ç å™¨
    decoder = Decoder(
        unet=unet,
        clip=clip,
        vae=vae if args.use_vqgan else None,
        image_sizes=(args.image_size,),
        timesteps=args.timesteps,
        image_cond_drop_prob=0.1,
        text_cond_drop_prob=0.5
    )
    
    return decoder


def create_dataloader(args):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print("ğŸ“ Creating data loader...")
    
    # å®šä¹‰æ•°æ®å˜æ¢
    transform = transforms.Compose([
        transforms.Resize((args.image_size, args.image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = create_micro_doppler_dataloader(
        data_root=args.data_root,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        image_size=args.image_size,
        num_users=args.num_users,
        shuffle=True,
        transform=transform,
        flat_structure=False  # ä½¿ç”¨å±‚æ¬¡ç»“æ„
    )
    
    return dataloader


def save_samples(decoder_trainer, epoch, output_dir, num_samples=8):
    """ç”Ÿæˆå¹¶ä¿å­˜æ ·æœ¬å›¾åƒ"""
    print(f"ğŸ¨ Generating samples for epoch {epoch}...")
    
    # åˆ›å»ºéšæœºå›¾åƒembeddingsç”¨äºé‡‡æ ·
    device = next(decoder_trainer.decoder.parameters()).device
    image_embeds = torch.randn(num_samples, 512, device=device)
    
    # ç”Ÿæˆæ ·æœ¬
    with torch.no_grad():
        samples = decoder_trainer.sample(image_embed=image_embeds)
    
    # ä¿å­˜æ ·æœ¬
    samples_dir = Path(output_dir) / 'samples'
    samples_dir.mkdir(exist_ok=True)
    
    for i, sample in enumerate(samples):
        # è½¬æ¢ä»[-1, 1]åˆ°[0, 1]
        sample = (sample + 1) / 2
        sample = torch.clamp(sample, 0, 1)
        
        # ä¿å­˜å›¾åƒ
        from torchvision.utils import save_image
        save_image(sample, samples_dir / f'epoch_{epoch:03d}_sample_{i:02d}.png')
    
    print(f"âœ… Saved {len(samples)} samples to {samples_dir}")


def main():
    args = parse_args()
    
    # æ£€æŸ¥Kaggleç¯å¢ƒ
    if not check_kaggle_environment():
        print("âŒ Environment check failed!")
        return 1
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.experiment_name is None:
        args.experiment_name = datetime.now().strftime("decoder_%Y%m%d_%H%M%S")
    
    output_dir = Path(args.output_dir) / args.experiment_name
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜å‚æ•°
    with open(output_dir / 'args.json', 'w') as f:
        json.dump(vars(args), f, indent=2)
    
    print(f"ğŸ“‚ Output directory: {output_dir}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Using device: {device}")

    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"ğŸ”§ GPU count: {gpu_count}")
        for i in range(gpu_count):
            print(f"ğŸ”§ GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"   Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB")

    # Acceleratoré…ç½® - æ·»åŠ DDPå‚æ•°å¤„ç†æœªä½¿ç”¨çš„å‚æ•°
    accelerator = Accelerator(
        mixed_precision='fp16',
        kwargs_handlers=[
            # å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒä¸­æœªä½¿ç”¨çš„å‚æ•°
            DistributedDataParallelKwargs(find_unused_parameters=True)
        ]
    )

    # åˆ›å»ºæ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨
    decoder = create_model(args)
    dataloader = create_dataloader(args)

    # ç§»åŠ¨æ¨¡å‹åˆ°GPU
    decoder = decoder.to(device)

    # å¤šGPUæ£€æµ‹ (è®©Acceleratorå¤„ç†å¤šGPU)
    if torch.cuda.device_count() > 1:
        print(f"ğŸ”¥ Multi-GPU detected: {torch.cuda.device_count()} GPUs (will use Accelerator)")
        # è°ƒæ•´æ‰¹æ¬¡å¤§å°ä»¥åˆ©ç”¨å¤šGPU
        if args.batch_size < torch.cuda.device_count() * 4:
            original_batch_size = args.batch_size
            args.batch_size = torch.cuda.device_count() * 8
            print(f"ğŸ”§ Adjusted batch size from {original_batch_size} to {args.batch_size} for multi-GPU")
            # é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨
            dataloader = create_dataloader(args)
    else:
        print("âš ï¸  Using single GPU")
    
    print(f"ğŸ“Š Dataset size: {len(dataloader.dataset)} images")
    print(f"ğŸ”¢ Batch size: {args.batch_size}")
    print(f"ğŸ“ˆ Total batches per epoch: {len(dataloader)}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    decoder_trainer = DecoderTrainer(
        decoder=decoder,
        lr=args.lr,
        wd=args.weight_decay,
        ema_beta=args.ema_beta,
        ema_update_after_step=args.ema_update_after_step,
        ema_update_every=args.ema_update_every,
        accelerator=accelerator
    )
    
    # å‡†å¤‡åˆ†å¸ƒå¼è®­ç»ƒ
    decoder_trainer, dataloader = accelerator.prepare(decoder_trainer, dataloader)

    # æ˜¾ç¤ºè®­ç»ƒé…ç½®
    print(f"ğŸš€ Starting training for {args.epochs} epochs")
    print(f"ğŸ”§ Accelerator device: {accelerator.device}")
    print(f"ğŸ”§ Number of processes: {accelerator.num_processes}")
    print(f"ğŸ”§ Distributed type: {accelerator.distributed_type}")
    if accelerator.num_processes > 1:
        print(f"ğŸ”¥ Multi-GPU training enabled with {accelerator.num_processes} GPUs!")
    else:
        print(f"âš ï¸  Single GPU training (check if multi-GPU is available)")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        decoder_trainer.train()
        
        epoch_loss = 0
        num_batches = 0
        
        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{args.epochs}')
        
        for batch in progress_bar:
            images = batch['image']
            
            # è®­ç»ƒæ­¥éª¤
            loss = decoder_trainer(images, unet_number=1)

            # å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„updateè°ƒç”¨
            if hasattr(decoder_trainer, 'module'):
                # åˆ†å¸ƒå¼è®­ç»ƒï¼šè®¿é—®åŸå§‹trainer
                decoder_trainer.module.update(unet_number=1)
            else:
                # å•GPUè®­ç»ƒï¼šç›´æ¥è°ƒç”¨
                decoder_trainer.update(unet_number=1)

            # losså·²ç»æ˜¯floatï¼Œä¸éœ€è¦.item()
            epoch_loss += loss
            num_batches += 1

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({'loss': f'{loss:.4f}'})
        
        avg_loss = epoch_loss / num_batches
        print(f'ğŸ“Š Epoch {epoch+1}/{args.epochs}, Average Loss: {avg_loss:.4f}')
        
        # ç”Ÿæˆæ ·æœ¬
        if (epoch + 1) % args.sample_every == 0:
            if accelerator.is_main_process:
                save_samples(decoder_trainer, epoch + 1, output_dir)
        
        # ä¿å­˜æ¨¡å‹
        if (epoch + 1) % args.save_every == 0:
            if accelerator.is_main_process:
                checkpoint = {
                    'epoch': epoch + 1,
                    'model_state_dict': decoder_trainer.state_dict(),
                    'args': vars(args)
                }
                torch.save(checkpoint, output_dir / f'decoder_epoch_{epoch+1:03d}.pt')
                print(f"ğŸ’¾ Saved checkpoint: epoch_{epoch+1:03d}.pt")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if accelerator.is_main_process:
        final_checkpoint = {
            'epoch': args.epochs,
            'model_state_dict': decoder_trainer.state_dict(),
            'args': vars(args)
        }
        torch.save(final_checkpoint, output_dir / 'decoder_final.pt')
        print(f"ğŸ‰ Training completed! Final model saved to {output_dir / 'decoder_final.pt'}")
    
    return 0


if __name__ == '__main__':
    exit(main())
