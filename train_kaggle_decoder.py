"""
Kaggleä¸“ç”¨è®­ç»ƒè„šæœ¬ - å¾®å¤šæ™®å‹’DALLE2è§£ç å™¨è®­ç»ƒ
é€‚é…Kaggleç¯å¢ƒå’Œæ•°æ®é›†ç»“æ„: /kaggle/input/dataset/ID_1 åˆ° ID_31
"""

import os
import json
import argparse
from pathlib import Path
from datetime import datetime
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from accelerate import Accelerator
from tqdm import tqdm

from dalle2_pytorch import Unet, Decoder, OpenClipAdapter
from dalle2_pytorch.trainer import DecoderTrainer
from dalle2_pytorch.dataloaders import create_micro_doppler_dataloader
from dalle2_pytorch.vqgan_vae import VQGanVAE, NullVQGanVAE


def parse_args():
    parser = argparse.ArgumentParser(description='Train Micro-Doppler DALLE2 Decoder on Kaggle')
    
    # Kaggleæ•°æ®è·¯å¾„ (å›ºå®š)
    parser.add_argument('--data_root', type=str, default='/kaggle/input/dataset',
                        help='Kaggle dataset root directory')
    parser.add_argument('--output_dir', type=str, default='/kaggle/working/outputs',
                        help='Output directory for models and logs')

    # æ•°æ®å‚æ•°
    parser.add_argument('--num_users', type=int, default=31,
                        help='Number of users (ID_1 to ID_31)')
    parser.add_argument('--image_size', type=int, default=256,
                        help='Image size (assumes square images)')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--dim', type=int, default=128,
                        help='Base dimension for U-Net (standard configuration)')
    parser.add_argument('--dim_mults', type=int, nargs='+', default=[1, 2, 4, 8],
                        help='Dimension multipliers for U-Net layers')
    parser.add_argument('--channels', type=int, default=3,
                        help='Number of image channels')
    parser.add_argument('--timesteps', type=int, default=1000,
                        help='Number of diffusion timesteps')
    parser.add_argument('--use_vqgan', action='store_true',
                        help='Use VQ-GAN VAE for latent diffusion')
    parser.add_argument('--no_vqgan', action='store_true',
                        help='Force disable VQ-GAN (use pixel-space diffusion)')
    parser.add_argument('--vq_codebook_size', type=int, default=512,
                        help='VQ-GAN codebook size (256/512/1024 for micro-Doppler data)')
    parser.add_argument('--aggressive_learning', action='store_true',
                        help='Use aggressive learning settings for faster convergence')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=8,
                        help='Batch size (Kaggle GPU memory limited)')
    parser.add_argument('--num_workers', type=int, default=2,
                        help='Number of data loader workers')
    parser.add_argument('--lr', type=float, default=3e-4,
                        help='Learning rate (increased for faster learning)')
    parser.add_argument('--weight_decay', type=float, default=1e-2,
                        help='Weight decay')
    parser.add_argument('--epochs', type=int, default=50,
                        help='Number of training epochs (reduced for Kaggle)')
    parser.add_argument('--save_every', type=int, default=10,
                        help='Save model every N epochs')
    parser.add_argument('--sample_every', type=int, default=1,
                        help='Generate samples every N epochs (1=every epoch for early monitoring)')
    
    # EMAå‚æ•°
    parser.add_argument('--ema_beta', type=float, default=0.99,
                        help='EMA decay rate')
    parser.add_argument('--ema_update_after_step', type=int, default=500,
                        help='Start EMA updates after N steps')
    parser.add_argument('--ema_update_every', type=int, default=10,
                        help='Update EMA every N steps')
    
    # CLIPå‚æ•°
    parser.add_argument('--clip_model', type=str, default='ViT-B/32',
                        help='CLIP model to use')
    
    # å®éªŒå‚æ•°
    parser.add_argument('--experiment_name', type=str, default=None,
                        help='Experiment name (default: timestamp)')
    parser.add_argument('--resume_from', type=str, default=None,
                        help='Resume training from checkpoint')
    
    return parser.parse_args()


def check_kaggle_environment():
    """æ£€æŸ¥Kaggleç¯å¢ƒå’Œæ•°æ®é›†"""
    print("ğŸ” Checking Kaggle environment...")
    
    # æ£€æŸ¥æ˜¯å¦åœ¨Kaggleç¯å¢ƒ
    if not os.path.exists('/kaggle'):
        print("âš ï¸  Warning: Not running in Kaggle environment")
        return False
    
    # æ£€æŸ¥æ•°æ®é›†è·¯å¾„
    data_path = Path('/kaggle/input/dataset')
    if not data_path.exists():
        print(f"âŒ Dataset not found at {data_path}")
        return False
    
    # æ£€æŸ¥ç”¨æˆ·æ–‡ä»¶å¤¹
    user_folders = []
    for i in range(1, 32):  # ID_1 to ID_31
        folder = data_path / f"ID_{i}"
        if folder.exists():
            user_folders.append(folder)
            # ç»Ÿè®¡å›¾åƒæ•°é‡
            image_count = len(list(folder.glob('*.png'))) + len(list(folder.glob('*.jpg')))
            print(f"âœ… Found ID_{i} with {image_count} images")
        else:
            print(f"âš ï¸  Missing ID_{i}")
    
    print(f"ğŸ“Š Total user folders found: {len(user_folders)}/31")
    return len(user_folders) > 0


def create_model(args):
    """åˆ›å»ºè§£ç å™¨æ¨¡å‹"""
    print("ğŸ—ï¸  Creating decoder model...")
    
    # åˆ›å»ºCLIPé€‚é…å™¨
    clip = OpenClipAdapter(args.clip_model)
    
    # åˆ›å»ºVQ-GAN VAE - å†…å­˜ä¼˜åŒ–é…ç½®
    if args.use_vqgan and not args.no_vqgan:
        print("ğŸ¨ Using VQ-GAN VAE for latent diffusion (memory optimized)")
        vae = VQGanVAE(
            dim=32,  # åŸºç¡€ç»´åº¦
            image_size=args.image_size,
            channels=args.channels,
            layers=3,  # æ ‡å‡†3å±‚: 256->128->64->32, encoded_dim=128
            vq_codebook_dim=256,  # VQ codebookç»´åº¦
            vq_codebook_size=args.vq_codebook_size,  # å¯é…ç½®çš„codebookå¤§å°
            vq_decay=0.8,  # æ ‡å‡†è¡°å‡ç‡
            vq_commitment_weight=1.0,  # æ ‡å‡†commitmentæƒé‡
            use_vgg_and_gan=False,  # ç¦ç”¨VGGå’ŒGANæŸå¤±é¿å…ä¸ç¨³å®š
            discr_layers=2,  # é€‚ä¸­çš„åˆ¤åˆ«å™¨å±‚æ•°
            attn_resolutions=[],  # ç¦ç”¨æ³¨æ„åŠ›èŠ‚çœå†…å­˜
        )
    else:
        print("ğŸ–¼ï¸  Using pixel-space diffusion")
        vae = NullVQGanVAE(channels=args.channels)
    
    # åˆ›å»ºU-Net - å§‹ç»ˆä½¿ç”¨3é€šé“ï¼ŒDecoderä¼šè‡ªåŠ¨è°ƒæ•´
    print(f"ğŸ”§ U-Net initial channels: {args.channels} (Decoder will auto-adjust for VQ-GAN)")
    if args.use_vqgan and not args.no_vqgan:
        print(f"ğŸ”§ VQ-GAN encoded_dim: {vae.encoded_dim} (will be used by Decoder)")

    unet = Unet(
        dim=args.dim,
        image_embed_dim=512,  # CLIP embedding dimension
        cond_dim=128,
        channels=args.channels,  # å§‹ç»ˆä½¿ç”¨3ï¼ŒDecoderä¼šè‡ªåŠ¨è°ƒæ•´
        dim_mults=tuple(args.dim_mults),
        cond_on_image_embeds=True,
        cond_on_text_encodings=False,  # ä¸ä½¿ç”¨æ–‡æœ¬æ¡ä»¶
        self_attn=True,  # å¯ç”¨è‡ªæ³¨æ„åŠ›
        attn_heads=8,  # æ ‡å‡†æ³¨æ„åŠ›å¤´æ•°
        attn_dim_head=64,  # æ ‡å‡†æ³¨æ„åŠ›ç»´åº¦
        cosine_sim_cross_attn=True,  # å¯ç”¨ä½™å¼¦ç›¸ä¼¼åº¦äº¤å‰æ³¨æ„åŠ›
        cosine_sim_self_attn=True   # å¯ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è‡ªæ³¨æ„åŠ›
    )
    
    # åˆ›å»ºè§£ç å™¨ - æ ¹æ®å­¦ä¹ æ¨¡å¼è°ƒæ•´é…ç½®
    if args.aggressive_learning:
        print("ğŸš€ Using aggressive learning settings")
        sample_timesteps = 64  # DDIMåŠ é€Ÿé‡‡æ ·
        image_cond_drop_prob = 0.2  # æ›´é«˜dropoutå¼ºåŒ–æ¡ä»¶å­¦ä¹ 
        beta_schedule = 'linear'  # çº¿æ€§è°ƒåº¦æ›´æ¿€è¿›
        predict_v = True  # ä½¿ç”¨v-parameterizationåŠ é€Ÿå­¦ä¹ 
    else:
        sample_timesteps = 64  # æ ‡å‡†DDIMé‡‡æ ·æ­¥æ•°
        image_cond_drop_prob = 0.1
        beta_schedule = 'cosine'
        predict_v = False

    decoder = Decoder(
        unet=unet,
        clip=clip,
        vae=vae if (args.use_vqgan and not args.no_vqgan) else None,
        image_sizes=(args.image_size,),
        timesteps=args.timesteps,
        sample_timesteps=sample_timesteps,
        image_cond_drop_prob=image_cond_drop_prob,
        text_cond_drop_prob=0.0,  # ä¸ä½¿ç”¨æ–‡æœ¬æ¡ä»¶
        beta_schedule=beta_schedule,
        predict_x_start=True,  # é¢„æµ‹x_startæ›´ç¨³å®š
        predict_v=predict_v,
        learned_variance=False  # å›ºå®šæ–¹å·®é¿å…å­¦ä¹ ä¸ç¨³å®š
    )
    
    return decoder


def create_dataloader(args):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print("ğŸ“ Creating data loader...")
    
    # å®šä¹‰æ•°æ®å˜æ¢
    transform = transforms.Compose([
        transforms.Resize((args.image_size, args.image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = create_micro_doppler_dataloader(
        data_root=args.data_root,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        image_size=args.image_size,
        num_users=args.num_users,
        shuffle=True,
        transform=transform,
        flat_structure=False  # ä½¿ç”¨å±‚æ¬¡ç»“æ„
    )
    
    return dataloader


def save_samples(decoder_trainer, epoch, output_dir, dataloader, num_samples=8):
    """ç”Ÿæˆå¹¶ä¿å­˜æ ·æœ¬å›¾åƒ"""
    print(f"ğŸ¨ Generating samples for epoch {epoch}...")

    device = next(decoder_trainer.decoder.parameters()).device

    # è·å–ä¸€æ‰¹çœŸå®å›¾åƒæ¥ç”ŸæˆCLIP embeddings
    try:
        batch = next(iter(dataloader))
        real_images = batch['image'][:num_samples].to(device)

        # ä½¿ç”¨CLIPç¼–ç çœŸå®å›¾åƒè·å¾—embeddings
        with torch.no_grad():
            clip = decoder_trainer.decoder.clip
            image_embeds, _ = clip.embed_image(real_images)

        print(f"ğŸ”§ Using real CLIP embeddings from {len(real_images)} images")

    except Exception as e:
        print(f"âš ï¸  Failed to get real embeddings: {e}")
        print(f"ğŸ”§ Falling back to random embeddings")
        # å›é€€åˆ°éšæœºembeddingsï¼Œä½†ä½¿ç”¨æ›´åˆç†çš„åˆ†å¸ƒ
        image_embeds = torch.randn(num_samples, 512, device=device) * 0.1

    # ç”Ÿæˆæ ·æœ¬
    with torch.no_grad():
        print(f"ğŸ”§ Image embeds shape: {image_embeds.shape}")
        print(f"ğŸ”§ Image embeds range: [{image_embeds.min().item():.3f}, {image_embeds.max().item():.3f}]")

        # åœ¨æ—©æœŸè®­ç»ƒæ—¶ä½¿ç”¨éEMAæ¨¡å‹
        use_non_ema = epoch <= 10  # å‰10ä¸ªepochä½¿ç”¨éEMAæ¨¡å‹
        print(f"ğŸ”§ Using {'non-EMA' if use_non_ema else 'EMA'} model for sampling")

        samples = decoder_trainer.sample(image_embed=image_embeds, use_non_ema=use_non_ema)

        print(f"ğŸ”§ Generated samples shape: {samples.shape}")
        print(f"ğŸ”§ Generated samples range: [{samples.min().item():.3f}, {samples.max().item():.3f}]")

    # ä¿å­˜æ ·æœ¬å’ŒåŸå›¾å¯¹æ¯”
    samples_dir = Path(output_dir) / 'samples'
    samples_dir.mkdir(exist_ok=True)

    # ä¿å­˜ç”Ÿæˆçš„æ ·æœ¬
    for i, sample in enumerate(samples):
        # æ£€æŸ¥åŸå§‹æ ·æœ¬å€¼
        print(f"ğŸ”§ Sample {i} raw range: [{sample.min().item():.3f}, {sample.max().item():.3f}]")

        # è½¬æ¢ä»[-1, 1]åˆ°[0, 1]
        sample = (sample + 1) / 2
        sample = torch.clamp(sample, 0, 1)

        print(f"ğŸ”§ Sample {i} after normalization: [{sample.min().item():.3f}, {sample.max().item():.3f}]")

        # ä¿å­˜å›¾åƒ
        from torchvision.utils import save_image
        save_image(sample, samples_dir / f'epoch_{epoch:03d}_generated_{i:02d}.png')

    # å¦‚æœæœ‰çœŸå®å›¾åƒï¼Œä¹Ÿä¿å­˜åŸå›¾ä½œä¸ºå¯¹æ¯”
    if 'real_images' in locals():
        for i, real_img in enumerate(real_images):
            real_img = (real_img + 1) / 2
            real_img = torch.clamp(real_img, 0, 1)
            from torchvision.utils import save_image
            save_image(real_img, samples_dir / f'epoch_{epoch:03d}_original_{i:02d}.png')

    print(f"âœ… Saved {len(samples)} samples to {samples_dir}")


def main():
    args = parse_args()
    
    # æ£€æŸ¥Kaggleç¯å¢ƒ
    if not check_kaggle_environment():
        print("âŒ Environment check failed!")
        return 1
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.experiment_name is None:
        args.experiment_name = datetime.now().strftime("decoder_%Y%m%d_%H%M%S")
    
    output_dir = Path(args.output_dir) / args.experiment_name
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜å‚æ•°
    with open(output_dir / 'args.json', 'w') as f:
        json.dump(vars(args), f, indent=2)
    
    print(f"ğŸ“‚ Output directory: {output_dir}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Using device: {device}")

    if torch.cuda.is_available():
        print(f"ğŸ”§ GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ”§ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

    # Acceleratoré…ç½® - ç¦ç”¨æ··åˆç²¾åº¦é¿å…NaNé—®é¢˜
    accelerator = Accelerator(mixed_precision='no')

    # åˆ›å»ºæ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨
    decoder = create_model(args)
    dataloader = create_dataloader(args)

    # ç§»åŠ¨æ¨¡å‹åˆ°GPU
    decoder = decoder.to(device)
    
    print(f"ğŸ“Š Dataset size: {len(dataloader.dataset)} images")
    print(f"ğŸ”¢ Batch size: {args.batch_size}")
    print(f"ğŸ“ˆ Total batches per epoch: {len(dataloader)}")
    
    # åˆ›å»ºè®­ç»ƒå™¨ - æ·»åŠ æ¢¯åº¦è£å‰ªé˜²æ­¢NaN
    decoder_trainer = DecoderTrainer(
        decoder=decoder,
        lr=args.lr,
        wd=args.weight_decay,
        ema_beta=args.ema_beta,
        ema_update_after_step=args.ema_update_after_step,
        ema_update_every=args.ema_update_every,
        max_grad_norm=1.0,  # å¼ºåˆ¶æ¢¯åº¦è£å‰ª
        accelerator=accelerator
    )
    
    # å‡†å¤‡åˆ†å¸ƒå¼è®­ç»ƒ
    decoder_trainer, dataloader = accelerator.prepare(decoder_trainer, dataloader)

    # æ˜¾ç¤ºè®­ç»ƒé…ç½®
    print(f"ğŸš€ Starting training for {args.epochs} epochs")
    print(f"ğŸ”§ Accelerator device: {accelerator.device}")
    print(f"ğŸ”§ Number of processes: {accelerator.num_processes}")
    print(f"ğŸ”§ Distributed type: {accelerator.distributed_type}")
    if accelerator.num_processes > 1:
        print(f"ğŸ”¥ Multi-GPU training enabled with {accelerator.num_processes} GPUs!")
    else:
        print(f"âš ï¸  Single GPU training (check if multi-GPU is available)")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        decoder_trainer.train()
        
        epoch_loss = 0
        num_batches = 0
        
        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{args.epochs}')
        
        for batch in progress_bar:
            images = batch['image']
            
            # è®­ç»ƒæ­¥éª¤
            loss = decoder_trainer(images, unet_number=1)

            # æ£€æŸ¥NaN
            if torch.isnan(torch.tensor(loss)):
                print(f"âŒ NaN loss detected at batch {num_batches}! Skipping...")
                continue

            # å•GPUè®­ç»ƒï¼šç›´æ¥è°ƒç”¨update
            decoder_trainer.update(unet_number=1)

            # losså·²ç»æ˜¯floatï¼Œä¸éœ€è¦.item()
            epoch_loss += loss
            num_batches += 1

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({'loss': f'{loss:.4f}'})
        
        avg_loss = epoch_loss / num_batches
        print(f'ğŸ“Š Epoch {epoch+1}/{args.epochs}, Average Loss: {avg_loss:.4f}')
        
        # ç”Ÿæˆæ ·æœ¬
        if (epoch + 1) % args.sample_every == 0:
            if accelerator.is_main_process:
                save_samples(decoder_trainer, epoch + 1, output_dir, dataloader)
        
        # ä¿å­˜æ¨¡å‹
        if (epoch + 1) % args.save_every == 0:
            if accelerator.is_main_process:
                checkpoint = {
                    'epoch': epoch + 1,
                    'model_state_dict': decoder_trainer.state_dict(),
                    'args': vars(args)
                }
                torch.save(checkpoint, output_dir / f'decoder_epoch_{epoch+1:03d}.pt')
                print(f"ğŸ’¾ Saved checkpoint: epoch_{epoch+1:03d}.pt")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if accelerator.is_main_process:
        final_checkpoint = {
            'epoch': args.epochs,
            'model_state_dict': decoder_trainer.state_dict(),
            'args': vars(args)
        }
        torch.save(final_checkpoint, output_dir / 'decoder_final.pt')
        print(f"ğŸ‰ Training completed! Final model saved to {output_dir / 'decoder_final.pt'}")
    
    return 0


if __name__ == '__main__':
    exit(main())
