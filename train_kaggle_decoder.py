"""
Kaggleä¸“ç”¨è®­ç»ƒè„šæœ¬ - å¾®å¤šæ™®å‹’DALLE2è§£ç å™¨è®­ç»ƒ
é€‚é…Kaggleç¯å¢ƒå’Œæ•°æ®é›†ç»“æ„: /kaggle/input/dataset/ID_1 åˆ° ID_31
"""

import os
import json
import argparse
from pathlib import Path
from datetime import datetime
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from accelerate import Accelerator
from tqdm import tqdm

from dalle2_pytorch import Unet, Decoder, OpenClipAdapter
from dalle2_pytorch.trainer import DecoderTrainer
from dalle2_pytorch.dataloaders import create_micro_doppler_dataloader
from dalle2_pytorch.vqgan_vae import VQGanVAE, NullVQGanVAE


def parse_args():
    parser = argparse.ArgumentParser(description='Train Micro-Doppler DALLE2 Decoder on Kaggle')
    
    # Kaggleæ•°æ®è·¯å¾„ (å›ºå®š)
    parser.add_argument('--data_root', type=str, default='/kaggle/input/dataset',
                        help='Kaggle dataset root directory')
    parser.add_argument('--output_dir', type=str, default='/kaggle/working/outputs',
                        help='Output directory for models and logs')

    # æ•°æ®å‚æ•°
    parser.add_argument('--num_users', type=int, default=31,
                        help='Number of users (ID1 to ID31)')
    parser.add_argument('--image_size', type=int, default=256,
                        help='Image size (assumes square images)')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--dim', type=int, default=128,
                        help='Base dimension for U-Net')
    parser.add_argument('--dim_mults', type=int, nargs='+', default=[1, 2, 4, 8],
                        help='Dimension multipliers for U-Net layers')
    parser.add_argument('--channels', type=int, default=3,
                        help='Number of image channels')
    parser.add_argument('--timesteps', type=int, default=1000,
                        help='Number of diffusion timesteps')
    parser.add_argument('--use_vqgan', action='store_true',
                        help='Use VQ-GAN VAE for latent diffusion')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=8,
                        help='Batch size (Kaggle GPU memory limited)')
    parser.add_argument('--num_workers', type=int, default=2,
                        help='Number of data loader workers')
    parser.add_argument('--lr', type=float, default=3e-4,
                        help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=1e-2,
                        help='Weight decay')
    parser.add_argument('--epochs', type=int, default=50,
                        help='Number of training epochs (reduced for Kaggle)')
    parser.add_argument('--save_every', type=int, default=10,
                        help='Save model every N epochs')
    parser.add_argument('--sample_every', type=int, default=5,
                        help='Generate samples every N epochs')
    
    # EMAå‚æ•°
    parser.add_argument('--ema_beta', type=float, default=0.99,
                        help='EMA decay rate')
    parser.add_argument('--ema_update_after_step', type=int, default=500,
                        help='Start EMA updates after N steps')
    parser.add_argument('--ema_update_every', type=int, default=10,
                        help='Update EMA every N steps')
    
    # CLIPå‚æ•°
    parser.add_argument('--clip_model', type=str, default='ViT-B/32',
                        help='CLIP model to use')
    
    # å®éªŒå‚æ•°
    parser.add_argument('--experiment_name', type=str, default=None,
                        help='Experiment name (default: timestamp)')
    parser.add_argument('--resume_from', type=str, default=None,
                        help='Resume training from checkpoint')
    
    return parser.parse_args()


def check_kaggle_environment():
    """æ£€æŸ¥Kaggleç¯å¢ƒå’Œæ•°æ®é›†"""
    print("ğŸ” Checking Kaggle environment...")
    
    # æ£€æŸ¥æ˜¯å¦åœ¨Kaggleç¯å¢ƒ
    if not os.path.exists('/kaggle'):
        print("âš ï¸  Warning: Not running in Kaggle environment")
        return False
    
    # æ£€æŸ¥æ•°æ®é›†è·¯å¾„
    data_path = Path('/kaggle/input/dataset')
    if not data_path.exists():
        print(f"âŒ Dataset not found at {data_path}")
        return False
    
    # æ£€æŸ¥ç”¨æˆ·æ–‡ä»¶å¤¹
    user_folders = []
    for i in range(1, 32):  # ID1 to ID31
        folder = data_path / f"ID{i}"
        if folder.exists():
            user_folders.append(folder)
            # ç»Ÿè®¡å›¾åƒæ•°é‡
            image_count = len(list(folder.glob('*.png'))) + len(list(folder.glob('*.jpg')))
            print(f"âœ… Found ID{i} with {image_count} images")
        else:
            print(f"âš ï¸  Missing ID{i}")
    
    print(f"ğŸ“Š Total user folders found: {len(user_folders)}/31")
    return len(user_folders) > 0


def create_model(args):
    """åˆ›å»ºè§£ç å™¨æ¨¡å‹"""
    print("ğŸ—ï¸  Creating decoder model...")
    
    # åˆ›å»ºCLIPé€‚é…å™¨
    clip = OpenClipAdapter(args.clip_model)
    
    # åˆ›å»ºVQ-GAN VAE (å¦‚æœæŒ‡å®š)
    if args.use_vqgan:
        print("ğŸ¨ Using VQ-GAN VAE for latent diffusion")
        vae = VQGanVAE(
            dim=32,
            image_size=args.image_size,
            channels=args.channels,
            layers=3,
            vq_codebook_dim=256,
            vq_codebook_size=1024,
            vq_decay=0.8,
            use_vgg_and_gan=True
        )
    else:
        print("ğŸ–¼ï¸  Using pixel-space diffusion")
        vae = NullVQGanVAE(channels=args.channels)
    
    # åˆ›å»ºU-Net
    unet = Unet(
        dim=args.dim,
        image_embed_dim=512,  # CLIP embedding dimension
        cond_dim=128,
        channels=args.channels,
        dim_mults=tuple(args.dim_mults),
        cond_on_image_embeds=True,
        cond_on_text_encodings=False,  # ä¸ä½¿ç”¨æ–‡æœ¬æ¡ä»¶
        self_attn=True,
        attn_heads=8,
        attn_dim_head=64,
        cosine_sim_cross_attn=True,
        cosine_sim_self_attn=True
    )
    
    # åˆ›å»ºè§£ç å™¨
    decoder = Decoder(
        unet=unet,
        clip=clip,
        vae=vae if args.use_vqgan else None,
        image_sizes=(args.image_size,),
        timesteps=args.timesteps,
        image_cond_drop_prob=0.1,
        text_cond_drop_prob=0.5
    )
    
    return decoder


def create_dataloader(args):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print("ğŸ“ Creating data loader...")
    
    # å®šä¹‰æ•°æ®å˜æ¢
    transform = transforms.Compose([
        transforms.Resize((args.image_size, args.image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = create_micro_doppler_dataloader(
        data_root=args.data_root,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        image_size=args.image_size,
        num_users=args.num_users,
        shuffle=True,
        transform=transform,
        flat_structure=False  # ä½¿ç”¨å±‚æ¬¡ç»“æ„
    )
    
    return dataloader


def save_samples(decoder_trainer, epoch, output_dir, num_samples=8):
    """ç”Ÿæˆå¹¶ä¿å­˜æ ·æœ¬å›¾åƒ"""
    print(f"ğŸ¨ Generating samples for epoch {epoch}...")
    
    # åˆ›å»ºéšæœºå›¾åƒembeddingsç”¨äºé‡‡æ ·
    device = next(decoder_trainer.decoder.parameters()).device
    image_embeds = torch.randn(num_samples, 512, device=device)
    
    # ç”Ÿæˆæ ·æœ¬
    with torch.no_grad():
        samples = decoder_trainer.sample(image_embed=image_embeds)
    
    # ä¿å­˜æ ·æœ¬
    samples_dir = Path(output_dir) / 'samples'
    samples_dir.mkdir(exist_ok=True)
    
    for i, sample in enumerate(samples):
        # è½¬æ¢ä»[-1, 1]åˆ°[0, 1]
        sample = (sample + 1) / 2
        sample = torch.clamp(sample, 0, 1)
        
        # ä¿å­˜å›¾åƒ
        from torchvision.utils import save_image
        save_image(sample, samples_dir / f'epoch_{epoch:03d}_sample_{i:02d}.png')
    
    print(f"âœ… Saved {len(samples)} samples to {samples_dir}")


def main():
    args = parse_args()
    
    # æ£€æŸ¥Kaggleç¯å¢ƒ
    if not check_kaggle_environment():
        print("âŒ Environment check failed!")
        return 1
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.experiment_name is None:
        args.experiment_name = datetime.now().strftime("decoder_%Y%m%d_%H%M%S")
    
    output_dir = Path(args.output_dir) / args.experiment_name
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜å‚æ•°
    with open(output_dir / 'args.json', 'w') as f:
        json.dump(vars(args), f, indent=2)
    
    print(f"ğŸ“‚ Output directory: {output_dir}")
    
    # åˆå§‹åŒ–accelerator
    accelerator = Accelerator(mixed_precision='fp16')  # ä½¿ç”¨æ··åˆç²¾åº¦èŠ‚çœå†…å­˜
    
    # åˆ›å»ºæ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨
    decoder = create_model(args)
    dataloader = create_dataloader(args)
    
    print(f"ğŸ“Š Dataset size: {len(dataloader.dataset)} images")
    print(f"ğŸ”¢ Batch size: {args.batch_size}")
    print(f"ğŸ“ˆ Total batches per epoch: {len(dataloader)}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    decoder_trainer = DecoderTrainer(
        decoder=decoder,
        lr=args.lr,
        wd=args.weight_decay,
        ema_beta=args.ema_beta,
        ema_update_after_step=args.ema_update_after_step,
        ema_update_every=args.ema_update_every,
        accelerator=accelerator
    )
    
    # å‡†å¤‡åˆ†å¸ƒå¼è®­ç»ƒ
    decoder_trainer, dataloader = accelerator.prepare(decoder_trainer, dataloader)
    
    print(f"ğŸš€ Starting training for {args.epochs} epochs")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        decoder_trainer.train()
        
        epoch_loss = 0
        num_batches = 0
        
        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{args.epochs}')
        
        for batch in progress_bar:
            images = batch['image']
            
            # è®­ç»ƒæ­¥éª¤
            loss = decoder_trainer(images, unet_number=1)
            decoder_trainer.update(unet_number=1)
            
            epoch_loss += loss.item()
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_loss = epoch_loss / num_batches
        print(f'ğŸ“Š Epoch {epoch+1}/{args.epochs}, Average Loss: {avg_loss:.4f}')
        
        # ç”Ÿæˆæ ·æœ¬
        if (epoch + 1) % args.sample_every == 0:
            if accelerator.is_main_process:
                save_samples(decoder_trainer, epoch + 1, output_dir)
        
        # ä¿å­˜æ¨¡å‹
        if (epoch + 1) % args.save_every == 0:
            if accelerator.is_main_process:
                checkpoint = {
                    'epoch': epoch + 1,
                    'model_state_dict': decoder_trainer.state_dict(),
                    'args': vars(args)
                }
                torch.save(checkpoint, output_dir / f'decoder_epoch_{epoch+1:03d}.pt')
                print(f"ğŸ’¾ Saved checkpoint: epoch_{epoch+1:03d}.pt")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if accelerator.is_main_process:
        final_checkpoint = {
            'epoch': args.epochs,
            'model_state_dict': decoder_trainer.state_dict(),
            'args': vars(args)
        }
        torch.save(final_checkpoint, output_dir / 'decoder_final.pt')
        print(f"ğŸ‰ Training completed! Final model saved to {output_dir / 'decoder_final.pt'}")
    
    return 0


if __name__ == '__main__':
    exit(main())
