#!/usr/bin/env python3
"""
è¶…è½»é‡çº§DALLE2æµ‹è¯• - æç®€é…ç½®é¿å…å†…å­˜é—®é¢˜
"""

import torch
import torch.nn as nn
from dalle2_pytorch import OpenAIClipAdapter, Unet, Decoder
import argparse

def create_ultra_lightweight_decoder():
    """åˆ›å»ºè¶…è½»é‡çº§è§£ç å™¨"""
    print("ğŸ”§ Creating ultra-lightweight decoder...")
    
    # ä½¿ç”¨é¢„è®­ç»ƒCLIP
    clip = OpenAIClipAdapter('ViT-B/32')
    print("âœ… CLIP loaded")
    
    # åˆ›å»ºæç®€U-Net
    unet = Unet(
        dim=16,                    # æå°ç»´åº¦
        image_embed_dim=512,       # CLIP embeddingç»´åº¦
        cond_dim=32,               # æå°æ¡ä»¶ç»´åº¦
        channels=3,
        dim_mults=(1,),            # åªæœ‰ä¸€å±‚ï¼Œä¸è¿›è¡Œä¸‹é‡‡æ ·
        cond_on_image_embeds=True,
        cond_on_text_encodings=False,
        # ç¦ç”¨æ‰€æœ‰å¯èƒ½çš„å†…å­˜æ¶ˆè€—åŠŸèƒ½
        memory_efficient=True,
        init_dim=None,
        init_conv_kernel_size=3,   # æ›´å°çš„å·ç§¯æ ¸
        resnet_groups=1,           # æœ€å°ç»„æ•°
        attn_dim_head=8,           # æå°æ³¨æ„åŠ›å¤´
        attn_heads=1,              # å•ä¸ªæ³¨æ„åŠ›å¤´
        ff_mult=1,                 # æœ€å°å‰é¦ˆå€æ•°
        layer_attns=False,         # ç¦ç”¨å±‚æ³¨æ„åŠ›
        layer_cross_attns=False,   # ç¦ç”¨äº¤å‰æ³¨æ„åŠ›
        use_sparse_linear_attn=False,  # ç¦ç”¨ç¨€ç–æ³¨æ„åŠ›
        block_kv_size=None,        # ç¦ç”¨å—æ³¨æ„åŠ›
        max_mem_len=0,             # ç¦ç”¨è®°å¿†
    )
    
    print(f"ğŸ”§ U-Net parameters: {sum(p.numel() for p in unet.parameters()):,}")
    
    # åˆ›å»ºè§£ç å™¨
    decoder = Decoder(
        unet=unet,
        clip=clip,
        vae=None,                  # åƒç´ ç©ºé—´
        image_sizes=(224,),        # å•ä¸€å°ºå¯¸
        timesteps=50,              # æå°‘æ—¶é—´æ­¥
        sample_timesteps=5,        # æå°‘é‡‡æ ·æ­¥æ•°
        image_cond_drop_prob=0.1,
        text_cond_drop_prob=0.0,   # ä¸ä½¿ç”¨æ–‡æœ¬æ¡ä»¶
        beta_schedule='linear',    # ç®€å•è°ƒåº¦
        predict_x_start=True,
        predict_v=False,
        learned_variance=False
    )
    
    print("âœ… Ultra-lightweight decoder created")
    return decoder

def test_step_by_step():
    """é€æ­¥æµ‹è¯•ï¼Œæ‰¾å‡ºå†…å­˜ç“¶é¢ˆ"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Using device: {device}")
    
    if not torch.cuda.is_available():
        print("âŒ No CUDA available")
        return False
    
    print(f"ğŸ”§ GPU memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB")
    torch.cuda.empty_cache()
    
    try:
        # æ­¥éª¤1: åˆ›å»ºæ¨¡å‹
        print("\n=== Step 1: Creating model ===")
        decoder = create_ultra_lightweight_decoder()
        print(f"ğŸ”§ Memory after model creation: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
        
        # æ­¥éª¤2: ç§»åŠ¨åˆ°GPU
        print("\n=== Step 2: Moving to GPU ===")
        decoder = decoder.to(device)
        print(f"ğŸ”§ Memory after GPU transfer: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
        
        # æ­¥éª¤3: åˆ›å»ºå°æ‰¹é‡æ•°æ®
        print("\n=== Step 3: Creating test data ===")
        batch_size = 1
        test_images = torch.randn(batch_size, 3, 224, 224, device=device)
        print(f"ğŸ”§ Memory after data creation: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
        
        # æ­¥éª¤4: å‰å‘ä¼ æ’­
        print("\n=== Step 4: Forward pass ===")
        with torch.no_grad():
            loss = decoder(test_images)
            print(f"âœ… Forward pass successful, loss: {loss.item():.4f}")
            print(f"ğŸ”§ Memory after forward: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
        
        # æ­¥éª¤5: é‡‡æ ·æµ‹è¯•
        print("\n=== Step 5: Sampling test ===")
        with torch.no_grad():
            # è·å–å›¾åƒembedding
            image_embed, _ = decoder.clip.embed_image(test_images)
            print(f"ğŸ”§ Image embed shape: {image_embed.shape}")
            print(f"ğŸ”§ Memory after embedding: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
            
            # å°è¯•é‡‡æ ·
            samples = decoder.sample(image_embed=image_embed)
            print(f"âœ… Sampling successful, shape: {samples.shape}")
            print(f"ğŸ”§ Memory after sampling: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error at step: {e}")
        print(f"ğŸ”§ Memory at error: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
        return False

def test_even_smaller():
    """æµ‹è¯•æ›´å°çš„é…ç½®"""
    print("\n=== Testing even smaller configuration ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    torch.cuda.empty_cache()
    
    try:
        # åªæµ‹è¯•CLIP
        print("Testing CLIP only...")
        clip = OpenAIClipAdapter('ViT-B/32')
        clip = clip.to(device)
        
        test_images = torch.randn(1, 3, 224, 224, device=device)
        with torch.no_grad():
            image_embed, _ = clip.embed_image(test_images)
            print(f"âœ… CLIP works, embed shape: {image_embed.shape}")
            print(f"ğŸ”§ Memory with CLIP only: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
        
        # æµ‹è¯•æœ€å°U-Net
        print("Testing minimal U-Net...")
        unet = Unet(
            dim=8,                     # æ›´å°
            image_embed_dim=512,
            cond_dim=16,               # æ›´å°
            channels=3,
            dim_mults=(1,),
            cond_on_image_embeds=True,
            cond_on_text_encodings=False,
            memory_efficient=True,
            attn_dim_head=4,           # æ›´å°
            attn_heads=1,
            ff_mult=1,
            layer_attns=False,
            layer_cross_attns=False,
        )
        
        print(f"ğŸ”§ Minimal U-Net parameters: {sum(p.numel() for p in unet.parameters()):,}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Even smaller config failed: {e}")
        return False

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--step_by_step', action='store_true', help='Test step by step')
    parser.add_argument('--even_smaller', action='store_true', help='Test even smaller config')
    args = parser.parse_args()
    
    print("ğŸš€ Ultra-lightweight DALLE2 test...")
    
    if args.step_by_step or not any([args.step_by_step, args.even_smaller]):
        success = test_step_by_step()
        if not success and not args.even_smaller:
            print("\nğŸ”§ Trying even smaller configuration...")
            test_even_smaller()
    
    if args.even_smaller:
        test_even_smaller()

if __name__ == "__main__":
    main()
